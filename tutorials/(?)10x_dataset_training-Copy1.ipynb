{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10X `tcellmatch` Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### NOTE:\n",
    "This tutorial probably won't work, use ``10x_dataset_preprocessing.ipynb`` and ``10x_dataset_tutorial.ipynb`` instead. This notebook simply contains additional visualization boilerplate code to use.\n",
    "\n",
    "This tutorial shows how to preprocess the raw data files from 10x database and feed into a feed forward network for categorical classification. The introduction of 10x can be found here: https://www.10xgenomics.com\n",
    "\n",
    "The task of this model is based on [this paper](https://www.embopress.org/doi/full/10.15252/msb.20199416), but, in brief, it approximates CDR3 RNA sequences to antigen specificity. This antigen specificity can further be predicted as either number of bindings or max bindings for each sequence.\n",
    "\n",
    "The architecture of the models are as follows\n",
    "\n",
    "## BiLSTM\n",
    "\n",
    "- Input Layer: n x (alpha/beta chain dimension) x CDR3 sequence length x one-hot encoding for each amino acid; here, the shape is `n x 1 x 40 x 26`\n",
    "\n",
    "- Hidden Layers:\n",
    "    1. **Embedding Layer**: An optional 1x1 convolutional layer which is used to create lower-dimensional embeddings of one-hot encoded amino acids before the sequence model is applied. The size of the embedding can be configured using the `aa_embedding_dim` parameter.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, aa_embedding_dim)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "    2. **Bi-directional Recurrent Layers**: A sequence of BiLSTM or BiGRU layers (configured via the `model` parameter), with a user-specified number of layers and dimensions. These layers process the sequence data and provide capability to capture complex temporal dependencies.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "    3. **Reshaping Layer**: The output from the BiLSTM/BiGRU layers is reshaped to be 2D in preparation for the fully connected layers, and non-sequence covariates, if provided, are concatenated with the sequence-derived representations.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        Output Shape: (batch_size, hidden_size * 2 \\[+ hidden_size * 2 if split\\] + num_covariates)\n",
    "\n",
    "    4. **Dense Layers**: A user-specified number of final fully connected layers (`depth_final_dense`) are used for the final task-specific prediction.\n",
    "\n",
    "        Input Shape: (batch_size, hidden_size * 2 \\[+ hidden_size * 2 if split\\] + num_covariates)\n",
    "\n",
    "        Output Shape: (batch_size, labels_dim)\n",
    "\n",
    "\n",
    "- Activation: ReLU or Softmax depending on task\n",
    "\n",
    "## Self-Attention\n",
    "\n",
    "- Input Layer: n x (alpha/beta chain dimension) x CDR3 sequence length x one-hot encoding for each amino acid; here, the shape is ``n x 1 x 40 x 26``\n",
    "- Hidden Layers:\n",
    "    1. **Embedding Layer**: An optional 1x1 convolutional layer which is used to create lower-dimensional embeddings of one-hot encoded amino acids before the sequence model is applied. The size of the embedding can be configured using the `aa_embedding_dim` parameter.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, aa_embedding_dim)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length, attention_size)\n",
    "\n",
    "    2. **Reshaping Layer**: The output from the self-attention layers is reshaped to be 2D in preparation for the fully connected layers, and non-sequence covariates, if provided, are concatenated with the sequence-derived representations\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, attention_size)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length * attention_size + num_covariates)\n",
    "    3. **Dense Layers**: A user-specified number of final fully connected layers (`depth_final_dense`) are used for the final task-specific prediction. \n",
    "\n",
    "        Input Shape: (batch_size, sequence_length * attention_size + num_covariates)\n",
    "\n",
    "        Output Shape: (batch_size, labels_dim)\n",
    "\n",
    "- Activation: ReLU or Softmax depending on task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tcellmatch.api as tm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the data below:\n",
    "\n",
    "https://www.10xgenomics.com/resources/datasets/cd-8-plus-t-cells-of-healthy-donor-1-1-standard-3-0-2\n",
    "\n",
    "https://www.10xgenomics.com/resources/datasets/cd-8-plus-t-cells-of-healthy-donor-2-1-standard-3-0-2\n",
    "\n",
    "For each donor, the binarized matrix corresponds to \"binarized matrix CSV\" on 10X and the clonotype matrix corresponds to \"VDJ - Clonotype info (CSV)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if running this on grace, get the TCR data from the data folder\n",
    "# !cp /data/datasets/sc10xTCRbinding/vdj_v1_hs_aggregated_donor1_binarized_matrix.csv ../tutorial_data/vdj_v1_hs_aggregated_donor1_binarized_matrix.csv \n",
    "# !cp /data/datasets/sc10xTCRbinding/vdj_v1_hs_aggregated_donor2_binarized_matrix.csv ../tutorial_data/vdj_v1_hs_aggregated_donor2_binarized_matrix.csv \n",
    "# !cp /data/datasets/sc10xTCRbinding/vdj_v1_hs_aggregated_donor1_clonotypes.csv ../tutorial_data/vdj_v1_hs_aggregated_donor1_clonotypes.csv \n",
    "# !cp /data/datasets/sc10xTCRbinding/vdj_v1_hs_aggregated_donor2_clonotypes.csv ../tutorial_data/vdj_v1_hs_aggregated_donor2_clonotypes.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of input directory.\n",
    "indir = '../tutorial_data/'\n",
    "# Path to 10x raw files.\n",
    "fns = [f\"{indir}vdj_v1_hs_aggregated_donor1_binarized_matrix.csv\",\n",
    "       f\"{indir}vdj_v1_hs_aggregated_donor2_binarized_matrix.csv\"]\n",
    "# Path to preprocessed clonotypes files.\n",
    "fns_clonotype = [f\"{indir}vdj_v1_hs_aggregated_donor1_clonotypes.csv\",\n",
    "                 f\"{indir}vdj_v1_hs_aggregated_donor2_clonotypes.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../tutorial_data/vdj_v1_hs_aggregated_donor1_binarized_matrix.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cellranger_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(fns[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      2\u001b[0m cellranger_out\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../tutorial_data/vdj_v1_hs_aggregated_donor1_binarized_matrix.csv'"
     ]
    }
   ],
   "source": [
    "cellranger_out = pd.read_csv(fns[0])\n",
    "cellranger_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cellranger_out\n",
    "column_names = data.columns\n",
    "column_types = data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellranger_out = pd.read_csv(fns_clonotype[0])\n",
    "cellranger_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of column names of labels to predict in 10x raw files\n",
    "Here we take all antigens from 10x dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = [\n",
    "    'A0101_VTEHDTLLY_IE-1_CMV_binder',\n",
    "    'A0201_KTWGQYWQV_gp100_Cancer_binder',\n",
    "    'A0201_ELAGIGILTV_MART-1_Cancer_binder',\n",
    "    'A0201_CLLWSFQTSA_Tyrosinase_Cancer_binder',\n",
    "    'A0201_IMDQVPFSV_gp100_Cancer_binder',\n",
    "    'A0201_SLLMWITQV_NY-ESO-1_Cancer_binder',\n",
    "    'A0201_KVAELVHFL_MAGE-A3_Cancer_binder',\n",
    "    'A0201_KVLEYVIKV_MAGE-A1_Cancer_binder',\n",
    "    'A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase_binder',\n",
    "    'A0201_LLDFVRFMGV_EBNA-3B_EBV_binder',\n",
    "    'A0201_LLMGTLGIVC_HPV-16E7_82-91_binder',\n",
    "    'A0201_CLGGLLTMV_LMP-2A_EBV_binder',\n",
    "    'A0201_YLLEMLWRL_LMP1_EBV_binder',\n",
    "    'A0201_FLYALALLL_LMP2A_EBV_binder',\n",
    "    'A0201_GILGFVFTL_Flu-MP_Influenza_binder',\n",
    "    'A0201_GLCTLVAML_BMLF1_EBV_binder',\n",
    "    'A0201_NLVPMVATV_pp65_CMV_binder',\n",
    "    'A0201_ILKEPVHGV_RT_HIV_binder',\n",
    "    'A0201_FLASKIGRLV_Ca2-indepen-Plip-A2_binder',\n",
    "    'A2402_CYTWNQMNL_WT1-(235-243)236M_Y_binder',\n",
    "    'A0201_RTLNAWVKV_Gag-protein_HIV_binder',\n",
    "    'A0201_KLQCVDLHV_PSA146-154_binder',\n",
    "    'A0201_LLFGYPVYV_HTLV-1_binder',\n",
    "    'A0201_SLFNTVATL_Gag-protein_HIV_binder',\n",
    "    'A0201_SLYNTVATLY_Gag-protein_HIV_binder',\n",
    "    'A0201_SLFNTVATLY_Gag-protein_HIV_binder',\n",
    "    'A0201_RMFPNAPYL_WT-1_binder',\n",
    "    'A0201_YLNDHLEPWI_BCL-X_Cancer_binder',\n",
    "    'A0201_MLDLQPETT_16E7_HPV_binder',\n",
    "    'A0301_KLGGALQAK_IE-1_CMV_binder',\n",
    "    'A0301_RLRAEAQVK_EMNA-3A_EBV_binder',\n",
    "    'A0301_RIAAWMATY_BCL-2L1_Cancer_binder',\n",
    "    'A1101_IVTDFSVIK_EBNA-3B_EBV_binder',\n",
    "    'A1101_AVFDRKSDAK_EBNA-3B_EBV_binder',\n",
    "    'B3501_IPSINVHHY_pp65_CMV_binder',\n",
    "    'A2402_AYAQKIFKI_IE-1_CMV_binder',\n",
    "    'A2402_QYDPVAALF_pp65_CMV_binder',\n",
    "    'B0702_QPRAPIRPI_EBNA-6_EBV_binder',\n",
    "    'B0702_TPRVTGGGAM_pp65_CMV_binder',\n",
    "    'B0702_RPPIFIRRL_EBNA-3A_EBV_binder',\n",
    "    'B0702_RPHERNGFTVL_pp65_CMV_binder',\n",
    "    'B0801_RAKFKQLL_BZLF1_EBV_binder',\n",
    "    'B0801_ELRRKMMYM_IE-1_CMV_binder',\n",
    "    'B0801_FLRGRAYGL_EBNA-3A_EBV_binder',\n",
    "    'A0101_SLEGGGLGY_NC_binder',\n",
    "    'A0101_STEGGGLAY_NC_binder',\n",
    "    'A0201_ALIAPVHAV_NC_binder',\n",
    "    'A2402_AYSSAGASI_NC_binder',\n",
    "    'B0702_GPAESAAGL_NC_binder',\n",
    "    'NR(B0801)_AAKGRGAAL_NC_binder'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of column names of negative controls in 10x raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_cols = [\n",
    "    'A0101_SLEGGGLGY_NC_binder',\n",
    "    'A0101_STEGGGLAY_NC_binder',\n",
    "    'A0201_ALIAPVHAV_NC_binder',\n",
    "    'A2402_AYSSAGASI_NC_binder',\n",
    "    'B0702_GPAESAAGL_NC_binder',\n",
    "    'NR(B0801)_AAKGRGAAL_NC_binder'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Use these if you want binding counts instead of top binder categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_BIND_COUNTS = True\n",
    "\n",
    "if USE_BIND_COUNTS:\n",
    "    target_ids = [\n",
    "        'A0101_VTEHDTLLY_IE-1_CMV',\n",
    "        'A0201_KTWGQYWQV_gp100_Cancer',\n",
    "        'A0201_ELAGIGILTV_MART-1_Cancer',\n",
    "        'A0201_CLLWSFQTSA_Tyrosinase_Cancer',\n",
    "        'A0201_IMDQVPFSV_gp100_Cancer',\n",
    "        'A0201_SLLMWITQV_NY-ESO-1_Cancer',\n",
    "        'A0201_KVAELVHFL_MAGE-A3_Cancer',\n",
    "        'A0201_KVLEYVIKV_MAGE-A1_Cancer',\n",
    "        'A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase',\n",
    "        'A0201_LLDFVRFMGV_EBNA-3B_EBV',\n",
    "        'A0201_LLMGTLGIVC_HPV-16E7_82-91',\n",
    "        'A0201_CLGGLLTMV_LMP-2A_EBV',\n",
    "        'A0201_YLLEMLWRL_LMP1_EBV',\n",
    "        'A0201_FLYALALLL_LMP2A_EBV',\n",
    "        'A0201_GILGFVFTL_Flu-MP_Influenza',\n",
    "        'A0201_GLCTLVAML_BMLF1_EBV',\n",
    "        'A0201_NLVPMVATV_pp65_CMV',\n",
    "        'A0201_ILKEPVHGV_RT_HIV',\n",
    "        'A0201_FLASKIGRLV_Ca2-indepen-Plip-A2',\n",
    "        'A2402_CYTWNQMNL_WT1-(235-243)236M_Y',\n",
    "        'A0201_RTLNAWVKV_Gag-protein_HIV',\n",
    "        'A0201_KLQCVDLHV_PSA146-154',\n",
    "        'A0201_LLFGYPVYV_HTLV-1',\n",
    "        'A0201_SLFNTVATL_Gag-protein_HIV',\n",
    "        'A0201_SLYNTVATLY_Gag-protein_HIV',\n",
    "        'A0201_SLFNTVATLY_Gag-protein_HIV',\n",
    "        'A0201_RMFPNAPYL_WT-1',\n",
    "        'A0201_YLNDHLEPWI_BCL-X_Cancer',\n",
    "        'A0201_MLDLQPETT_16E7_HPV',\n",
    "        'A0301_KLGGALQAK_IE-1_CMV',\n",
    "        'A0301_RLRAEAQVK_EMNA-3A_EBV',\n",
    "        'A0301_RIAAWMATY_BCL-2L1_Cancer',\n",
    "        'A1101_IVTDFSVIK_EBNA-3B_EBV',\n",
    "        'A1101_AVFDRKSDAK_EBNA-3B_EBV',\n",
    "        'B3501_IPSINVHHY_pp65_CMV',\n",
    "        'A2402_AYAQKIFKI_IE-1_CMV',\n",
    "        'A2402_QYDPVAALF_pp65_CMV',\n",
    "        'B0702_QPRAPIRPI_EBNA-6_EBV',\n",
    "        'B0702_TPRVTGGGAM_pp65_CMV',\n",
    "        'B0702_RPPIFIRRL_EBNA-3A_EBV',\n",
    "        'B0702_RPHERNGFTVL_pp65_CMV',\n",
    "        'B0801_RAKFKQLL_BZLF1_EBV',\n",
    "        'B0801_ELRRKMMYM_IE-1_CMV',\n",
    "        'B0801_FLRGRAYGL_EBNA-3A_EBV',\n",
    "        'A0101_SLEGGGLGY_NC',\n",
    "        'A0101_STEGGGLAY_NC',\n",
    "        'A0201_ALIAPVHAV_NC',\n",
    "        'A2402_AYSSAGASI_NC',\n",
    "        'B0702_GPAESAAGL_NC',\n",
    "        'NR(B0801)_AAKGRGAAL_NC',\n",
    "    ]\n",
    "    nc_cols = [\n",
    "        'A0101_SLEGGGLGY_NC',\n",
    "        'A0101_STEGGGLAY_NC',\n",
    "        'A0201_ALIAPVHAV_NC',\n",
    "        'A2402_AYSSAGASI_NC',\n",
    "        'B0702_GPAESAAGL_NC',\n",
    "        'NR(B0801)_AAKGRGAAL_NC'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model object\n",
    "EstimatorFfn() includes all of reading, training and testing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = tm.models.EstimatorFfn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read 10x raw files, taking out TCR CDR3 and binarized binding matrix as training data\n",
    "We encode the TCR CDR3 amino acid sequences (include TRA and TRB) with one-hot encoding, the embedded sequences are of shape [num_samples, tra/trb, max_sequence_length, aa_onehot_dim]. For example if we take out 4000 TRB sequences seperately, the maximal length of sequences is 30 and we have 22 amino acids, the shape of output would be [4000, 1, 30, 26]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ffn.read_binarized_matrix(\n",
    "    fns=[indir + x for x in fns],\n",
    "    fns_clonotype=[indir + x for x in fns_clonotype],\n",
    "    fns_covar=[],\n",
    "    fn_blosum=f\"{indir}BLOSUM50.csv\",\n",
    "    blosum_encoding=False,\n",
    "    is_train=True,\n",
    "    # include categorical variable for which donor we get from\n",
    "    covariate_formula_categ=[\"donor\"],\n",
    "    covariate_formula_numeric=[],\n",
    "    # Whether to add an additional non-binder category for softmax activation function\n",
    "    add_non_binder_for_softmax=False,\n",
    "    # we are only keeping trb chain\n",
    "    chains=\"trb\",\n",
    "    label_cols=target_ids,\n",
    "    nc_cols=nc_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input consists of TCR CDR3 sequences and covariates. Covariates act as a additional information which will be concatenated with TCR CDR3 sequences during training. The target set is a binarized matrix which shows the binding between TCR CDR3 and antigens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR sequences: \",ffn.x_train.shape)\n",
    "# print(\"The head of TCR sequences: \",ffn.x_train[0])\n",
    "print(\"Shape of covariates: \",ffn.covariates_train.shape)\n",
    "# print(\"The head of covariates: \",ffn.covariates_train[0:5])\n",
    "print(\"Shape of target set: \",ffn.y_train.shape)\n",
    "# print(\"The head of target set: \",ffn.y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample clonotypes to data stored in x_train\n",
    "This avoids training, evaluation or test set being too biased toward a subset of TCRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_obs: Maximum number of observations per clonotype.\n",
    "ffn.downsample_clonotype(max_obs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Shape of TCR sequences: \",ffn.x_train.shape)\n",
    "print(\"Shape of covariates: \",ffn.covariates_train.shape)\n",
    "print(\"Shape of target set: \",ffn.y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create test dataset\n",
    "We can either split the training set or use a new database as the test set. Here, we split test set from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.clear_test_data()\n",
    "ffn.sample_test_set(test_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR CDR3 sequences for training: \",ffn.x_train.shape)\n",
    "print(\"Shape of covariates for training: \",ffn.covariates_train.shape)\n",
    "print(\"Shape of target set for training: \",ffn.y_train.shape)\n",
    "print(\"Shape of TCR CDR3 sequences for test: \",ffn.x_test.shape)\n",
    "print(\"Shape of covariates for test: \",ffn.covariates_test.shape)\n",
    "print(\"Shape of target set for test: \",ffn.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding TCR CDR3 sequences in both training and testing set\n",
    "Since we can use TCR CDR3 in another database as the test set, we should make sure they have same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.pad_sequence(target_len=40, sequence=\"tcr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downsample data to given number of observations.\n",
    "In order to save time, we sample a small dataset for training. Never use this method in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffn.downsample_data(n=200, data=\"train\")\n",
    "# ffn.downsample_data(n=200, data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR CDR3 sequences for training: \",ffn.x_train.shape)\n",
    "print(\"Shape of TCR CDR3 sequences for test: \",ffn.x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save imported data as NumPy Arrays (optional)\n",
    "This can be useful if you want to avoid binarizing when debugging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TO_NUMPY = True\n",
    "if SAVE_TO_NUMPY:\n",
    "    np.savez_compressed(\n",
    "        f\"{indir}ffn_data.npz\",\n",
    "        x_train=ffn.x_train,\n",
    "        covariates_train=ffn.covariates_train,\n",
    "        y_train=ffn.y_train,\n",
    "        x_test=ffn.x_test,\n",
    "        covariates_test=ffn.covariates_test,\n",
    "        y_test=ffn.y_test,\n",
    "        clone_train=ffn.clone_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "Here, we can build two models: a BiLSTM-based or self-attention-based model, detailed above.\n",
    "\n",
    "Loss has the following possible parameters\n",
    "\n",
    "1. Discrete\n",
    "    1. Binary Crossentropy (param \"bce\")\n",
    "    2. Weighted Binary Crossentropy (param \"wbce\")\n",
    "    3. Categorical Crossentropy (param \"cce\")\n",
    "2. Continuous\n",
    "    1. MMD (param \"mmd\")\n",
    "    2. Mean Squared error (param \"mse\")\n",
    "    3. Poisson (param \"pois\")\n",
    "    \n",
    "Calling one of these creates the model and sets the ffn.model attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SELF_ATTENTION = False\n",
    "lr = 0.005\n",
    "aa_embedding_dim = 0 # = 26\n",
    "depth_final_dense = 3\n",
    "\n",
    "if USE_SELF_ATTENTION:\n",
    "    attention_size = [128, 128]\n",
    "    attention_heads = [16, 16]\n",
    "    params = {\"attention_size\" : attention_size,\n",
    "              \"attention_heads\" : attention_heads}\n",
    "    ffn.build_self_attention(\n",
    "        residual_connection=True,\n",
    "        aa_embedding_dim=aa_embedding_dim,\n",
    "        # hidden size of each attention layer\n",
    "        attention_size=attention_size,\n",
    "        # number of heads at each layer\n",
    "        use_covariates=False,\n",
    "        attention_heads=attention_heads,\n",
    "        depth_final_dense=depth_final_dense,\n",
    "        optimizer='adam',\n",
    "        lr=lr,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wbce',\n",
    "        label_smoothing=0\n",
    "    )\n",
    "else:\n",
    "    topology = [32, 32]#[128, 128, 128, 128]\n",
    "    params = {\"topology\" : topology}\n",
    "    ffn.build_bilstm(\n",
    "        # The depth of each bilstm layer (length of feature vector)\n",
    "        topology = topology,\n",
    "        residual_connection=True,\n",
    "        # Dimension of the linear amino acid embedding, ie number of 1x1 convolutional filters.\n",
    "        # set to input dimension if aa_embedding_dim==0.\n",
    "        aa_embedding_dim=aa_embedding_dim,\n",
    "        # depth_final_dense=depth_final_dense,\n",
    "        optimizer='adam',\n",
    "        lr=lr,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wcbe',\n",
    "        label_smoothing=0,\n",
    "        depth_final_dense=depth_final_dense,\n",
    "        # whether to assume covariates in model architecture\n",
    "        use_covariates=False,\n",
    "        # whether we are predicting max binding categorical\n",
    "        # or binding counts\n",
    "        one_hot_y=not USE_BIND_COUNTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Train this model for 2 epochs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(target_ids)[[13, 14, 15, 33, 34, 41]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "batch_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"TCR fitting\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Self Attention\" if USE_SELF_ATTENTION else \"BiLSTM\",\n",
    "    \"batch_size\": batch_size,\n",
    "    \"depth_final_dense\" : depth_final_dense,\n",
    "    **params,\n",
    "    },\n",
    "    reinit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_curve, val_curve, antigen_loss, antigen_loss_val = ffn.train(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=batch_size,\n",
    "    # tensorboard logs to this directory\n",
    "    log_dir='training_runs',\n",
    "    # if true, saves epochs x n_classes as ffn.antigen_loss\n",
    "    # ijth element is loss of ith epoch on jth antigen\n",
    "    save_antigen_loss=True,\n",
    "    allow_early_stopping=False,\n",
    "    print_loss=True,\n",
    "    lr_schedule_factor=0.99999,\n",
    "    use_wandb=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(xs):\n",
    "    return ffn.model.get_embeddings(torch.from_numpy(xs).to(torch.float32).to(ffn.device)).detach().cpu().numpy()\n",
    "def get_preds(xs):\n",
    "    return ffn.model(torch.from_numpy(xs).to(torch.float32).to(ffn.device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "burn_in_epochs = 3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 5])\n",
    "# Plotting the curves\n",
    "n_batches = int(np.ceil(len(ffn.x_train)/batch_size))\n",
    "step_per_epoch = len(train_curve) // EPOCHS\n",
    "xs = np.linspace(burn_in_epochs, EPOCHS, (EPOCHS-burn_in_epochs)*n_batches)\n",
    "plt.plot(xs, np.array(train_curve)[step_per_epoch * burn_in_epochs:], label='Train Curve')\n",
    "plt.plot(np.arange(burn_in_epochs, EPOCHS),\n",
    "    np.array(val_curve)[burn_in_epochs:], label='Validation Curve')\n",
    "plt.xlim(burn_in_epochs-0.5, EPOCHS+0.5)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=20)\n",
    "plt.title('Training and Validation Curves', fontsize=20)\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "wandb.log({\"loss chart plot\": ax})\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "### plot training losses per antigen\n",
    "burn_in_epochs = 0\n",
    "xs = np.arange(burn_in_epochs, EPOCHS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 5])\n",
    "# Plotting the curves\n",
    "colors = cm.viridis(np.linspace(0, 1, antigen_loss.shape[1]))\n",
    "for curve, color in zip(np.array(antigen_loss)[burn_in_epochs:].T, colors):\n",
    "    plt.plot(xs, curve, color=color)\n",
    "# plt.plot(np.array(antigen_loss_val)[10:], label='Validation Curves')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Absolute Loss')\n",
    "plt.title('Training curves per antigen')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"train antigen loss chart\": ax})\n",
    "\n",
    "### plot relative losses\n",
    "burn_in_epochs = 0\n",
    "xs = np.arange(burn_in_epochs, EPOCHS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 5])\n",
    "# Plotting the curves\n",
    "colors = cm.viridis(np.linspace(0, 1, antigen_loss.shape[1]))\n",
    "for curve, color in zip(np.array(antigen_loss)[burn_in_epochs:].T, colors):\n",
    "    plt.plot(xs, curve/curve[0], color=color)\n",
    "# plt.plot(np.array(antigen_loss_val)[10:], label='Validation Curves')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Relative Loss')\n",
    "plt.title('Training curves per antigen')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"train antigen rel loss chart\": ax})\n",
    "\n",
    "### plot testing losses per antigen\n",
    "burn_in_epochs = 0\n",
    "xs = np.arange(burn_in_epochs, EPOCHS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 5])\n",
    "# Plotting the curves\n",
    "colors = cm.viridis(np.linspace(0, 1, antigen_loss.shape[1]))\n",
    "for curve, color in zip(np.array(antigen_loss_val)[burn_in_epochs:].T, colors):\n",
    "    plt.plot(xs, curve, color=color)\n",
    "# plt.plot(np.array(antigen_loss_val)[10:], label='Validation Curves')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Absolute Loss')\n",
    "plt.title('Testing curves per antigen')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"test antigen loss chart\": ax})\n",
    "\n",
    "### plot relative losses\n",
    "\n",
    "# plot relative losses\n",
    "burn_in_epochs = 0\n",
    "xs = np.arange(burn_in_epochs, EPOCHS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 5])\n",
    "# Plotting the curves\n",
    "colors = cm.viridis(np.linspace(0, 1, antigen_loss.shape[1]))\n",
    "for curve, color in zip(np.array(antigen_loss_val)[burn_in_epochs:].T, colors):\n",
    "    plt.plot(xs, curve/curve[0], color=color)\n",
    "# plt.plot(np.array(antigen_loss_val)[10:], label='Validation Curves')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Relative Loss')\n",
    "plt.title('Testing curves per antigen')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"test antigen rel loss chart\": ax})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot average likelihoods under Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO import interacitve plots into wandb\n",
    "\n",
    "import torch.nn as nn\n",
    "from scipy.special import loggamma\n",
    "loss = nn.PoissonNLLLoss(log_input=True, full=True)\n",
    "\n",
    "ffn.predict()\n",
    "ffn.predictions.shape\n",
    "\n",
    "z = ffn.y_test\n",
    "log_z_hat = ffn.predictions\n",
    "z_hat = np.exp(log_z_hat)\n",
    "log_pois = -z_hat + z * log_z_hat - loggamma(z+1)\n",
    "av_log_pois = log_pois.mean(axis=0)\n",
    "\n",
    "trivial_pred = np.average(z, axis=0)\n",
    "log_pois_trivial = -trivial_pred + z * np.log(trivial_pred) - loggamma(z+1)\n",
    "av_log_pois_trivial = log_pois_trivial.mean(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 5])\n",
    "plt.title(\"testing data\")\n",
    "ax.bar(np.arange(len(av_log_pois))-0.25, av_log_pois, width=0.5)\n",
    "ax.bar(np.arange(len(av_log_pois_trivial))+0.25, av_log_pois_trivial, width=0.5)\n",
    "plt.ylabel(\"average log probability\")\n",
    "plt.xlabel(\"antigen\")\n",
    "\n",
    "wandb.log({\"test antigen liks to trivial\": wandb.Image(plt)})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 5])\n",
    "plt.title(\"testing data\")\n",
    "plt.plot([-10, len(av_log_pois)+10], [1, 1], color='grey')\n",
    "ax.bar(np.arange(len(av_log_pois)), av_log_pois / av_log_pois_trivial, width=0.5)\n",
    "plt.ylabel(\"relative average log probability\")\n",
    "plt.xlabel(\"antigen\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, len(av_log_pois)-0.5)\n",
    "\n",
    "wandb.log({\"test antigen liks ratio to trivial\": wandb.Image(plt)})\n",
    "\n",
    "\n",
    "\n",
    "z_t = ffn.y_train\n",
    "log_z_hat_t = np.concatenate([get_preds(ffn.x_train[batch_size*i:batch_size*(i+1)])\n",
    "                               for i in range(n_batches+1)])\n",
    "z_hat_t = np.exp(log_z_hat_t)\n",
    "log_pois_t = -z_hat_t + z_t * log_z_hat_t - loggamma(z_t+1)\n",
    "av_log_pois_t = log_pois_t.mean(axis=0)\n",
    "# print(\"average log likelihood:\\t\\t\\t\\t\", av_log_pois)\n",
    "\n",
    "trivial_pred_t = np.average(z_t, axis=0)\n",
    "log_pois_trivial_t = -trivial_pred_t + z_t * np.log(trivial_pred_t) - loggamma(z_t+1)\n",
    "av_log_pois_trivial_t = log_pois_trivial_t.mean(axis=0)\n",
    "# print(\"average log likelihood from trivial pred:\\t\", av_log_pois_trivial)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 5])\n",
    "plt.title(\"training data\")\n",
    "plt.bar(np.arange(len(av_log_pois_t))-0.25, av_log_pois_t, width=0.5)\n",
    "plt.bar(np.arange(len(av_log_pois_trivial_t))+0.25, av_log_pois_trivial_t, width=0.5)\n",
    "plt.ylabel(\"average log probability\")\n",
    "plt.xlabel(\"antigen\")\n",
    "\n",
    "wandb.log({\"train antigen liks to trivial\": wandb.Image(plt)})\n",
    "\n",
    "\n",
    "wandb.log({\"test antigen liks to trivial\": wandb.Image(plt)})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 5])\n",
    "plt.title(\"training data\")\n",
    "plt.plot([-10, len(av_log_pois)+10], [1, 1], color='grey')\n",
    "ax.bar(np.arange(len(av_log_pois)), av_log_pois_t / av_log_pois_trivial_t, width=0.5)\n",
    "plt.ylabel(\"relative average log probability\")\n",
    "plt.xlabel(\"antigen\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, len(av_log_pois)-0.5)\n",
    "\n",
    "wandb.log({\"train antigen liks ratio to trivial\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot predictions vs real binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from scipy.special import loggamma\n",
    "loss = nn.PoissonNLLLoss(log_input=True, full=True)\n",
    "\n",
    "for ind, target_id in enumerate(target_ids):\n",
    "\n",
    "    print(\"average log likelihood:\\t\\t\\t\\t\", av_log_pois[ind])\n",
    "    print(\"average log likelihood from trivial pred:\\t\", av_log_pois_trivial[ind])\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=[5, 10])\n",
    "    ax[0].plot(z[:, ind], z_hat[:, ind], '.')\n",
    "    max_x = np.max(z[:, ind])\n",
    "    max_y = np.max(z_hat[:, ind])\n",
    "    ax[0].set_xlim(0-0.01-max_x*0.01, max_x*1.01+0.01)\n",
    "    ax[0].set_ylim(0-0.01-max_y*0.01, max_y*1.01+0.01)\n",
    "    ax[0].set_ylabel(\"predicted binding\")\n",
    "    ax[0].set_xlabel(\"true binding\")\n",
    "    ax[0].set_title(str(ind)+'_'+target_id)\n",
    "    ax[0].plot([-max_x, 2*max_x], np.average(z[:, ind])*np.ones(2), color='orange')\n",
    "    ax[0].plot([0, max_x+max_y], [0, max_x+max_y], color='grey')\n",
    "\n",
    "    ax[1].plot(np.log(1+z[:, ind]), np.log(1+z_hat[:, ind]), '.')\n",
    "    max_x = np.max(np.log(1+z[:, ind]))\n",
    "    max_y = np.max(np.log(1+z_hat[:, ind]))\n",
    "    ax[1].set_xlim(0-0.01-max_x*0.01, max_x*1.01+0.01)\n",
    "    ax[1].set_ylim(0-0.01-max_y*0.01, max_y*1.01+0.01)\n",
    "    ax[1].set_ylabel(\"predicted log binding\")\n",
    "    ax[1].set_xlabel(\"true log binding\")\n",
    "    ax[1].set_title(str(ind)+'_'+target_id)\n",
    "    ax[1].plot([-max_x, 2*max_x], np.log(1+np.average(z[:, ind]))*np.ones(2), color='orange')\n",
    "    ax[1].plot([0, max_x+max_y], [0, max_x+max_y], color='grey')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    wandb.log({\"antigen_plots/antigen {}\".format(ind): wandb.Image(plt)})\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot representation of counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_t = np.concatenate([get_embeddings(ffn.x_train[batch_size*i:batch_size*(i+1)])\n",
    "                               for i in range(n_batches+1)])\n",
    "embeddings = get_embeddings(ffn.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "n_pca_components = 10\n",
    "\n",
    "scs = np.real(np.linalg.eig(np.dot(embeddings.T, embeddings))[1])\n",
    "pca = np.dot(embeddings, scs[:, :n_pca_components])\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=20).fit_transform(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "for ind, target_id in enumerate(target_ids):\n",
    "\n",
    "    dat = np.log(z[:, ind]+1)\n",
    "    dat = (dat)/(np.max(dat))\n",
    "    \n",
    "    dat_hat = np.log(z_hat[:, ind]+1)\n",
    "    dat_hat = (dat_hat)/(np.max(dat_hat))\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=[5, 10])\n",
    "    ax[0].scatter(X_embedded[:, 0], X_embedded[:, 1], color=cm.viridis(dat), s=6)\n",
    "    ax[0].set_ylabel(\"tsne_2\", fontsize=20)\n",
    "    ax[0].set_title(str(ind)+'_'+target_id + \" real binding\")\n",
    "    \n",
    "    ax[1].scatter(X_embedded[:, 0], X_embedded[:, 1], color=cm.viridis(dat_hat), s=6)\n",
    "    ax[1].set_xlabel(\"tsne_1\", fontsize=20)\n",
    "    ax[1].set_ylabel(\"tsne_2\", fontsize=20)\n",
    "    ax[1].set_title(str(ind)+'_'+target_id + \" pred binding\")\n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"antigen_rep_plots/antigen rep {}\".format(ind): wandb.Image(plt)})\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(f'{indir}{timestamp}.saved_model', exist_ok=True)\n",
    "# save_yhat means save predictions\n",
    "ffn.save_model_full(f'{indir}{timestamp}.saved_model', save_yhat=False, save_train_data=True)\n",
    "\n",
    "np.save(f'{indir}{timestamp}.saved_model/preds.npy', np.r_[z_hat_t, z_hat])\n",
    "np.save(f'{indir}{timestamp}.saved_model/embeddings.npy', np.r_[embeddings_t, embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reinit instead\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce evaluation in a new instance of model w/ same weights\n",
    "\n",
    "We load the model, with weights and data included, and evaluate and predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seqs_ohe = np.concatenate([ffn.x_train, ffn.x_test], axis=0)[:, 0, :, :-1].astype(bool)\n",
    "all_ys = np.log(1 + np.concatenate([ffn.y_train, ffn.y_test], axis=0))\n",
    "\n",
    "seqs = all_seqs_ohe\n",
    "ys = all_ys\n",
    "n_cov = ys.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.r_[embeddings_t, embeddings].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.r_[z_hat_t, z_hat].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn2.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ffn2 = tm.models.EstimatorFfn()\n",
    "ffn2.load_model_full(fn=f'{indir}{timestamp}.saved_model')\n",
    "# print(ffn2.evaluate(test_only=True))\n",
    "# # saves to ffn2.predictions\n",
    "# ffn2.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.get_residuals(14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jboesen tcellmatch",
   "language": "python",
   "name": "jboesen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
