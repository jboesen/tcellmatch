{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec9e301-2d64-437c-ae70-e4b6dc4ed647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tcellmatch.api as tm\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "from math import exp, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b25653-1282-4e22-b732-eed7837b7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = '../tutorial_data/saved_model-8-2--18:56'\n",
    "ffn = tm.models.EstimatorFfn()\n",
    "ffn.load_model_full(fn=saved_model_path, load_train_data=False)\n",
    "\n",
    "device = 'cuda' if tc.cuda.is_available() else 'cpu'\n",
    "# device='cpu'\n",
    "ffn.model = ffn.model.to(device) \n",
    "for param in ffn.model.parameters():\n",
    "    param = param.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f68677b-9334-4399-a531-7e53b4abe6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ce054f-a905-4fb7-9f97-3ec5b33b727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_pois(tau, y_hat, y):\n",
    "    y_hat, y = tc.Tensor(y_hat).to(device), tc.Tensor(y).to(device)\n",
    "    num_samples = 1000\n",
    "    samples_idx = tc.randperm(y_hat.size(0))[:num_samples]\n",
    "    y_hat, y = y_hat[samples_idx], y[samples_idx]\n",
    "    # acct for log lmd\n",
    "    y_hat = tc.exp(y_hat)\n",
    "    # broadcast to vectorize pairwise output\n",
    "    y_hat_i = y_hat[:, None, :]  # Add a new dimension to treat i and j differently\n",
    "    y_hat_j = y_hat[None, :, :]  # Add a new dimension to treat i and j differently\n",
    "    p_lmd = tc.nn.PoissonNLLLoss(log_input=False, full=True)\n",
    "    \n",
    "    dist_vals = 2 * tc.square(y_hat_i - y_hat_j)\n",
    "\n",
    "    def k(y, y_):\n",
    "        # take gss to be y\n",
    "        return tc.exp(-1/(2 * y) * (tc.log(y + 1) - tc.log(y_ + 1)) ** 2)\n",
    "\n",
    "    def calc_norm(y_hat, y):\n",
    "        n, n_antigens = y_hat.shape\n",
    "        max_lambdas = y_hat.max(dim=0)[0]\n",
    "        max_lambdas[max_lambdas < 5] = 5\n",
    "        max_lambdas = max_lambdas**2.\n",
    "        out = tc.zeros(n_antigens, device=device)\n",
    "\n",
    "        for col in range(n_antigens):\n",
    "            max_lambda = int(max_lambdas[col])\n",
    "            # Prepare a tensor of λ values\n",
    "            lambda_values = tc.arange(1, max_lambda+1).float().to(device)\n",
    "\n",
    "            # Compute the loss for all combinations of y and λ\n",
    "            \n",
    "            losses = tc.zeros((n, max_lambda))\n",
    "            for i in range(n):\n",
    "                for j in range(max_lambda):\n",
    "                    losses[i,j] = p_lmd(lambda_values[j], y[i, col])\n",
    "\n",
    "            # Reshape to (n, 1, Λ, 1) and (1, n, 1, Λ) to use broadcasting\n",
    "            # i_mtx = losses[:, None, :, None]\n",
    "            i_mtx = losses.reshape(n, 1, max_lambda, 1)\n",
    "            # j_mtx = losses[None, :, None, :]\n",
    "            j_mtx = losses.reshape(1, n, 1, max_lambda)\n",
    "\n",
    "            # the ijklth element is L(y_i | λ = λ̂_k) * L(y_j | λ = λ̂_l)\n",
    "            A = (i_mtx * j_mtx).to(device)\n",
    "\n",
    "            # k(λ̂_i, λ̂_j) for all (i, j)\n",
    "            K = k(lambda_values.reshape(max_lambda, 1), lambda_values.reshape(1, max_lambda))\n",
    "\n",
    "            # broadcast K to shape n x n x lmd x lmd\n",
    "            K_broadcasted = K[None, None, :, :].to(device=device)\n",
    "            E_both = (A * K_broadcasted).sum()\n",
    "\n",
    "            # p_lmd(y_i | λ = j)\n",
    "            losses = p_lmd(lambda_values[None, :], y[:, col, None])\n",
    "\n",
    "            # one-d E_{y\\sim\\mu(X_i)}k(y, y_j)\n",
    "            k_Pois = k(y[:, col, None], lambda_values[None, :])\n",
    "            one_d_E = (losses * k_Pois).sum()\n",
    "\n",
    "            last_term = K.sum()\n",
    "            # one_d_E looks at each pair twice\n",
    "            # and we are looking at the same column, so this is the same as \n",
    "            # -(E_{y\\sim\\mu(X_i)}k(y, y_j)+E_{y'\\sim\\mu(X_j)}k(y_i, y'))\n",
    "            out[col] = E_both - one_d_E + last_term\n",
    "        return out.sum()\n",
    "    return exp(-1/(2 * tau ** 2)) * dist_vals.sum() + calc_norm(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d48340-e607-4356-abca-811256c96ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2482e+12, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmd_pois(1, ffn.predictions, ffn.y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jboesen tcellmatch",
   "language": "python",
   "name": "jboesen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
