{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10X `tcellmatch` Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to preprocess the raw data files from 10x database and feed into a feed forward network for categorical classification. The introduction of 10x can be found here: https://www.10xgenomics.com\n",
    "\n",
    "The task of this model is based on [this paper](https://www.embopress.org/doi/full/10.15252/msb.20199416), but, in brief, it approximates CDR3 RNA sequences to antigen specificity. This antigen specificity can further be predicted as either number of bindings or max bindings for each sequence.\n",
    "\n",
    "The architecture of the models are as follows\n",
    "\n",
    "## BiLSTM\n",
    "\n",
    "- Input Layer: n x (alpha/beta chain dimension) x CDR3 sequence length x one-hot encoding for each amino acid; here, the shape is `n x 1 x 40 x 26`\n",
    "\n",
    "- Hidden Layers:\n",
    "    1. **Embedding Layer**: An optional 1x1 convolutional layer which is used to create lower-dimensional embeddings of one-hot encoded amino acids before the sequence model is applied. The size of the embedding can be configured using the `aa_embedding_dim` parameter.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, aa_embedding_dim)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "    2. **Bi-directional Recurrent Layers**: A sequence of BiLSTM or BiGRU layers (configured via the `model` parameter), with a user-specified number of layers and dimensions. These layers process the sequence data and provide capability to capture complex temporal dependencies.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "    3. **Reshaping Layer**: The output from the BiLSTM/BiGRU layers is reshaped to be 2D in preparation for the fully connected layers, and non-sequence covariates, if provided, are concatenated with the sequence-derived representations.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        Output Shape: (batch_size, hidden_size * 2 \\[+ hidden_size * 2 if split\\] + num_covariates)\n",
    "\n",
    "    4. **Dense Layers**: A user-specified number of final fully connected layers (`depth_final_dense`) are used for the final task-specific prediction.\n",
    "\n",
    "        Input Shape: (batch_size, hidden_size * 2 \\[+ hidden_size * 2 if split\\] + num_covariates)\n",
    "\n",
    "        Output Shape: (batch_size, labels_dim)\n",
    "\n",
    "\n",
    "- Activation: ReLU or Softmax depending on task\n",
    "\n",
    "## Self-Attention\n",
    "\n",
    "- Input Layer: n x (alpha/beta chain dimension) x CDR3 sequence length x one-hot encoding for each amino acid; here, the shape is ``n x 1 x 40 x 26``\n",
    "- Hidden Layers:\n",
    "    1. **Embedding Layer**: An optional 1x1 convolutional layer which is used to create lower-dimensional embeddings of one-hot encoded amino acids before the sequence model is applied. The size of the embedding can be configured using the `aa_embedding_dim` parameter.\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, aa_embedding_dim)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length, attention_size)\n",
    "\n",
    "    2. **Reshaping Layer**: The output from the self-attention layers is reshaped to be 2D in preparation for the fully connected layers, and non-sequence covariates, if provided, are concatenated with the sequence-derived representations\n",
    "\n",
    "        Input Shape: (batch_size, sequence_length, attention_size)\n",
    "\n",
    "        Output Shape: (batch_size, sequence_length * attention_size + num_covariates)\n",
    "    3. **Dense Layers**: A user-specified number of final fully connected layers (`depth_final_dense`) are used for the final task-specific prediction. \n",
    "\n",
    "        Input Shape: (batch_size, sequence_length * attention_size + num_covariates)\n",
    "\n",
    "        Output Shape: (batch_size, labels_dim)\n",
    "\n",
    "- Activation: ReLU or Softmax depending on task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 11:45:39.893052: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-12 11:45:39.943532: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 11:45:40.689925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import tcellmatch.api as tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the data below:\n",
    "\n",
    "https://www.10xgenomics.com/resources/datasets/cd-8-plus-t-cells-of-healthy-donor-1-1-standard-3-0-2\n",
    "\n",
    "https://www.10xgenomics.com/resources/datasets/cd-8-plus-t-cells-of-healthy-donor-2-1-standard-3-0-2\n",
    "\n",
    "For each donor, the binarized matrix corresponds to \"binarized matrix CSV\" on 10X and the clonotype matrix corresponds to \"VDJ - Clonotype info (CSV)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of input directory.\n",
    "indir = '../tutorial_data/'\n",
    "# Path to 10x raw files.\n",
    "fns = [f\"{indir}vdj_v1_hs_aggregated_donor1_binarized_matrix.csv\",\n",
    "       f\"{indir}vdj_v1_hs_aggregated_donor2_binarized_matrix.csv\"]\n",
    "# Path to preprocessed clonotypes files.\n",
    "fns_clonotype = [f\"{indir}vdj_v1_hs_aggregated_donor1_clonotypes.csv\",\n",
    "                 f\"{indir}vdj_v1_hs_aggregated_donor2_clonotypes.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../tutorial_data/vdj_v1_hs_aggregated_donor1_binarized_matrix.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cellranger_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(fns[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      2\u001b[0m cellranger_out\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/jboesen/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../tutorial_data/vdj_v1_hs_aggregated_donor1_binarized_matrix.csv'"
     ]
    }
   ],
   "source": [
    "cellranger_out = pd.read_csv(fns[0])\n",
    "cellranger_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cellranger_out\n",
    "column_names = data.columns\n",
    "column_types = data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellranger_out = pd.read_csv(fns_clonotype[0])\n",
    "cellranger_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of column names of labels to predict in 10x raw files\n",
    "Here we take all antigens from 10x dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = [\n",
    "    'A0101_VTEHDTLLY_IE-1_CMV_binder',\n",
    "    'A0201_KTWGQYWQV_gp100_Cancer_binder',\n",
    "    'A0201_ELAGIGILTV_MART-1_Cancer_binder',\n",
    "    'A0201_CLLWSFQTSA_Tyrosinase_Cancer_binder',\n",
    "    'A0201_IMDQVPFSV_gp100_Cancer_binder',\n",
    "    'A0201_SLLMWITQV_NY-ESO-1_Cancer_binder',\n",
    "    'A0201_KVAELVHFL_MAGE-A3_Cancer_binder',\n",
    "    'A0201_KVLEYVIKV_MAGE-A1_Cancer_binder',\n",
    "    'A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase_binder',\n",
    "    'A0201_LLDFVRFMGV_EBNA-3B_EBV_binder',\n",
    "    'A0201_LLMGTLGIVC_HPV-16E7_82-91_binder',\n",
    "    'A0201_CLGGLLTMV_LMP-2A_EBV_binder',\n",
    "    'A0201_YLLEMLWRL_LMP1_EBV_binder',\n",
    "    'A0201_FLYALALLL_LMP2A_EBV_binder',\n",
    "    'A0201_GILGFVFTL_Flu-MP_Influenza_binder',\n",
    "    'A0201_GLCTLVAML_BMLF1_EBV_binder',\n",
    "    'A0201_NLVPMVATV_pp65_CMV_binder',\n",
    "    'A0201_ILKEPVHGV_RT_HIV_binder',\n",
    "    'A0201_FLASKIGRLV_Ca2-indepen-Plip-A2_binder',\n",
    "    'A2402_CYTWNQMNL_WT1-(235-243)236M_Y_binder',\n",
    "    'A0201_RTLNAWVKV_Gag-protein_HIV_binder',\n",
    "    'A0201_KLQCVDLHV_PSA146-154_binder',\n",
    "    'A0201_LLFGYPVYV_HTLV-1_binder',\n",
    "    'A0201_SLFNTVATL_Gag-protein_HIV_binder',\n",
    "    'A0201_SLYNTVATLY_Gag-protein_HIV_binder',\n",
    "    'A0201_SLFNTVATLY_Gag-protein_HIV_binder',\n",
    "    'A0201_RMFPNAPYL_WT-1_binder',\n",
    "    'A0201_YLNDHLEPWI_BCL-X_Cancer_binder',\n",
    "    'A0201_MLDLQPETT_16E7_HPV_binder',\n",
    "    'A0301_KLGGALQAK_IE-1_CMV_binder',\n",
    "    'A0301_RLRAEAQVK_EMNA-3A_EBV_binder',\n",
    "    'A0301_RIAAWMATY_BCL-2L1_Cancer_binder',\n",
    "    'A1101_IVTDFSVIK_EBNA-3B_EBV_binder',\n",
    "    'A1101_AVFDRKSDAK_EBNA-3B_EBV_binder',\n",
    "    'B3501_IPSINVHHY_pp65_CMV_binder',\n",
    "    'A2402_AYAQKIFKI_IE-1_CMV_binder',\n",
    "    'A2402_QYDPVAALF_pp65_CMV_binder',\n",
    "    'B0702_QPRAPIRPI_EBNA-6_EBV_binder',\n",
    "    'B0702_TPRVTGGGAM_pp65_CMV_binder',\n",
    "    'B0702_RPPIFIRRL_EBNA-3A_EBV_binder',\n",
    "    'B0702_RPHERNGFTVL_pp65_CMV_binder',\n",
    "    'B0801_RAKFKQLL_BZLF1_EBV_binder',\n",
    "    'B0801_ELRRKMMYM_IE-1_CMV_binder',\n",
    "    'B0801_FLRGRAYGL_EBNA-3A_EBV_binder',\n",
    "    'A0101_SLEGGGLGY_NC_binder',\n",
    "    'A0101_STEGGGLAY_NC_binder',\n",
    "    'A0201_ALIAPVHAV_NC_binder',\n",
    "    'A2402_AYSSAGASI_NC_binder',\n",
    "    'B0702_GPAESAAGL_NC_binder',\n",
    "    'NR(B0801)_AAKGRGAAL_NC_binder'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of column names of negative controls in 10x raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_cols = [\n",
    "    'A0101_SLEGGGLGY_NC_binder',\n",
    "    'A0101_STEGGGLAY_NC_binder',\n",
    "    'A0201_ALIAPVHAV_NC_binder',\n",
    "    'A2402_AYSSAGASI_NC_binder',\n",
    "    'B0702_GPAESAAGL_NC_binder',\n",
    "    'NR(B0801)_AAKGRGAAL_NC_binder'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Use these if you want binding counts instead of top binder categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_BIND_COUNTS = True\n",
    "\n",
    "if USE_BIND_COUNTS:\n",
    "    target_ids = [\n",
    "        'A0101_VTEHDTLLY_IE-1_CMV',\n",
    "        'A0201_KTWGQYWQV_gp100_Cancer',\n",
    "        'A0201_ELAGIGILTV_MART-1_Cancer',\n",
    "        'A0201_CLLWSFQTSA_Tyrosinase_Cancer',\n",
    "        'A0201_IMDQVPFSV_gp100_Cancer',\n",
    "        'A0201_SLLMWITQV_NY-ESO-1_Cancer',\n",
    "        'A0201_KVAELVHFL_MAGE-A3_Cancer',\n",
    "        'A0201_KVLEYVIKV_MAGE-A1_Cancer',\n",
    "        'A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase',\n",
    "        'A0201_LLDFVRFMGV_EBNA-3B_EBV',\n",
    "        'A0201_LLMGTLGIVC_HPV-16E7_82-91',\n",
    "        'A0201_CLGGLLTMV_LMP-2A_EBV',\n",
    "        'A0201_YLLEMLWRL_LMP1_EBV',\n",
    "        'A0201_FLYALALLL_LMP2A_EBV',\n",
    "        'A0201_GILGFVFTL_Flu-MP_Influenza',\n",
    "        'A0201_GLCTLVAML_BMLF1_EBV',\n",
    "        'A0201_NLVPMVATV_pp65_CMV',\n",
    "        'A0201_ILKEPVHGV_RT_HIV',\n",
    "        'A0201_FLASKIGRLV_Ca2-indepen-Plip-A2',\n",
    "        'A2402_CYTWNQMNL_WT1-(235-243)236M_Y',\n",
    "        'A0201_RTLNAWVKV_Gag-protein_HIV',\n",
    "        'A0201_KLQCVDLHV_PSA146-154',\n",
    "        'A0201_LLFGYPVYV_HTLV-1',\n",
    "        'A0201_SLFNTVATL_Gag-protein_HIV',\n",
    "        'A0201_SLYNTVATLY_Gag-protein_HIV',\n",
    "        'A0201_SLFNTVATLY_Gag-protein_HIV',\n",
    "        'A0201_RMFPNAPYL_WT-1',\n",
    "        'A0201_YLNDHLEPWI_BCL-X_Cancer',\n",
    "        'A0201_MLDLQPETT_16E7_HPV',\n",
    "        'A0301_KLGGALQAK_IE-1_CMV',\n",
    "        'A0301_RLRAEAQVK_EMNA-3A_EBV',\n",
    "        'A0301_RIAAWMATY_BCL-2L1_Cancer',\n",
    "        'A1101_IVTDFSVIK_EBNA-3B_EBV',\n",
    "        'A1101_AVFDRKSDAK_EBNA-3B_EBV',\n",
    "        'B3501_IPSINVHHY_pp65_CMV',\n",
    "        'A2402_AYAQKIFKI_IE-1_CMV',\n",
    "        'A2402_QYDPVAALF_pp65_CMV',\n",
    "        'B0702_QPRAPIRPI_EBNA-6_EBV',\n",
    "        'B0702_TPRVTGGGAM_pp65_CMV',\n",
    "        'B0702_RPPIFIRRL_EBNA-3A_EBV',\n",
    "        'B0702_RPHERNGFTVL_pp65_CMV',\n",
    "        'B0801_RAKFKQLL_BZLF1_EBV',\n",
    "        'B0801_ELRRKMMYM_IE-1_CMV',\n",
    "        'B0801_FLRGRAYGL_EBNA-3A_EBV',\n",
    "        'A0101_SLEGGGLGY_NC',\n",
    "        'A0101_STEGGGLAY_NC',\n",
    "        'A0201_ALIAPVHAV_NC',\n",
    "        'A2402_AYSSAGASI_NC',\n",
    "        'B0702_GPAESAAGL_NC',\n",
    "        'NR(B0801)_AAKGRGAAL_NC',\n",
    "    ]\n",
    "    nc_cols = [\n",
    "        'A0101_SLEGGGLGY_NC',\n",
    "        'A0101_STEGGGLAY_NC',\n",
    "        'A0201_ALIAPVHAV_NC',\n",
    "        'A2402_AYSSAGASI_NC',\n",
    "        'B0702_GPAESAAGL_NC',\n",
    "        'NR(B0801)_AAKGRGAAL_NC'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model object\n",
    "EstimatorFfn() includes all of reading, training and testing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = tm.models.EstimatorFfn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read 10x raw files, taking out TCR CDR3 and binarized binding matrix as training data\n",
    "We encode the TCR CDR3 amino acid sequences (include TRA and TRB) with one-hot encoding, the embedded sequences are of shape [num_samples, tra/trb, max_sequence_length, aa_onehot_dim]. For example if we take out 4000 TRB sequences seperately, the maximal length of sequences is 30 and we have 22 amino acids, the shape of output would be [4000, 1, 30, 26]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.read_binarized_matrix(\n",
    "    fns=[indir + x for x in fns],\n",
    "    fns_clonotype=[indir + x for x in fns_clonotype],\n",
    "    fns_covar=[],\n",
    "    fn_blosum=f\"{indir}BLOSUM50.csv\",\n",
    "    blosum_encoding=False,\n",
    "    is_train=True,\n",
    "    # include categorical variable for which donor we get from\n",
    "    covariate_formula_categ=[\"donor\"],\n",
    "    covariate_formula_numeric=[],\n",
    "    # Whether to add an additional non-binder category for softmax activation function\n",
    "    add_non_binder_for_softmax=False,\n",
    "    # we are only keeping trb chain\n",
    "    chains=\"trb\",\n",
    "    label_cols=target_ids,\n",
    "    nc_cols=nc_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input consists of TCR CDR3 sequences and covariates. Covariates act as a additional information which will be concatenated with TCR CDR3 sequences during training. The target set is a binarized matrix which shows the binding between TCR CDR3 and antigens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR sequences: \",ffn.x_train.shape)\n",
    "# print(\"The head of TCR sequences: \",ffn.x_train[0])\n",
    "print(\"Shape of covariates: \",ffn.covariates_train.shape)\n",
    "# print(\"The head of covariates: \",ffn.covariates_train[0:5])\n",
    "print(\"Shape of target set: \",ffn.y_train.shape)\n",
    "# print(\"The head of target set: \",ffn.y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample clonotypes to data stored in x_train\n",
    "This avoids training, evaluation or test set being too biased toward a subset of TCRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_obs: Maximum number of observations per clonotype.\n",
    "ffn.downsample_clonotype(max_obs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR sequences: \",ffn.x_train.shape)\n",
    "print(\"Shape of covariates: \",ffn.covariates_train.shape)\n",
    "print(\"Shape of target set: \",ffn.y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test dataset\n",
    "We can either split the training set or use a new database as the test set. Here, we split test set from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.clear_test_data()\n",
    "ffn.sample_test_set(test_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR CDR3 sequences for training: \",ffn.x_train.shape)\n",
    "print(\"Shape of covariates for training: \",ffn.covariates_train.shape)\n",
    "print(\"Shape of target set for training: \",ffn.y_train.shape)\n",
    "print(\"Shape of TCR CDR3 sequences for test: \",ffn.x_test.shape)\n",
    "print(\"Shape of covariates for test: \",ffn.covariates_test.shape)\n",
    "print(\"Shape of target set for test: \",ffn.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding TCR CDR3 sequences in both training and testing set\n",
    "Since we can use TCR CDR3 in another database as the test set, we should make sure they have same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.pad_sequence(target_len=40, sequence=\"tcr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample data to given number of observations.\n",
    "In order to save time, we sample a small dataset for training. Never use this method in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffn.downsample_data(n=200, data=\"train\")\n",
    "# ffn.downsample_data(n=200, data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of TCR CDR3 sequences for training: \",ffn.x_train.shape)\n",
    "print(\"Shape of TCR CDR3 sequences for test: \",ffn.x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save imported data as NumPy Arrays (optional)\n",
    "This can be useful if you want to avoid binarizing when debugging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_TO_NUMPY = True\n",
    "# if SAVE_TO_NUMPY:\n",
    "#     np.savez_compressed(\n",
    "#         f\"{indir}ffn_data_downsampled.npz\",\n",
    "#         x_train=ffn.x_train,\n",
    "#         covariates_train=ffn.covariates_train,\n",
    "#         y_train=ffn.y_train,\n",
    "#         x_test=ffn.x_test,\n",
    "#         covariates_test=ffn.covariates_test,\n",
    "#         y_test=ffn.y_test,\n",
    "#         clone_train=ffn.clone_train\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "Here, we can build two models: a BiLSTM-based or self-attention-based model, detailed above.\n",
    "\n",
    "Loss has the following possible parameters\n",
    "\n",
    "1. Discrete\n",
    "    1. Binary Crossentropy (param \"bce\")\n",
    "    2. Weighted Binary Crossentropy (param \"wbce\")\n",
    "    3. Categorical Crossentropy (param \"cce\")\n",
    "2. Continuous\n",
    "    1. MMD (param \"mmd\")\n",
    "    2. Mean Squared error (param \"mse\")\n",
    "    3. Poisson (param \"pois\")\n",
    "    \n",
    "Calling one of these creates the model and sets the ffn.model attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SELF_ATTENTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SELF_ATTENTION:\n",
    "    ffn.build_self_attention(\n",
    "        residual_connection=True,\n",
    "        aa_embedding_dim=0,\n",
    "        # hidden size of each attention layer\n",
    "        attention_size=[5, 5],\n",
    "        # number of heads at each layer\n",
    "        attention_heads=[4, 4],\n",
    "        optimizer='adam',\n",
    "        lr=0.001,\n",
    "        loss='mmd' if USE_BIND_COUNTS else 'wbce',\n",
    "        label_smoothing=0\n",
    "    )\n",
    "else:\n",
    "    ffn.build_bilstm(\n",
    "        # The depth of each bilstm layer (length of feature vector)\n",
    "        topology = [10, 10, 10, 10],\n",
    "        residual_connection=True,\n",
    "        # Dimension of the linear amino acid embedding, ie number of 1x1 convolutional filters.\n",
    "        # set to input dimension if aa_embedding_dim==0.\n",
    "        aa_embedding_dim=0,\n",
    "        optimizer='adam',\n",
    "        lr=0.001,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wcbe',\n",
    "        label_smoothing=0,\n",
    "        # whether to assume covariates in model architecture\n",
    "        use_covariates=False,\n",
    "        # whether we are predicting max binding categorical\n",
    "        # or binding counts\n",
    "        one_hot_y=not USE_BIND_COUNTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Train this model for 2 epochs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 35\n",
    "train_curve, val_curve = ffn.train(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=8,\n",
    "    # tensorboard logs to this directory\n",
    "    log_dir='training_runs',\n",
    "    # if true, saves epochs x n_classes as ffn.antigen_loss\n",
    "    # ijth element is loss of ith epoch on jth antigen\n",
    "    save_antigen_loss=False,\n",
    "    allow_early_stopping=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curves\n",
    "plt.plot(train_curve, label='Train Curve')\n",
    "plt.plot(val_curve, label='Validation Curve')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Curves')\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to get the ouputs before our linear layer and store them for other use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.model.get_embeddings(torch.ones(1, 1, 40, 26)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set\n",
    "This evaluates the data and returns binary and custom (i.e., based on how the model was built above) loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.evaluate(\n",
    "    # given k, returns loss only over the kth antigen in test\n",
    "    antigen_col=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{indir}saved_model', exist_ok=True)\n",
    "# save_yhat means save predictions\n",
    "ffn.save_model_full(f'{indir}saved_model', save_yhat=True, save_train_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate/Compare the Model\n",
    "We predict the labels on test data and store to `ffn.predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.predict()\n",
    "ffn.predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix\n",
    "This is only applicable, of course, if you've used the one-hot encoded maximum binding y-data. If it is, you can use this to also compare with the original tcellmatch in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_BIND_COUNTS:\n",
    "    true_labels = np.argmax(ffn.y_test, axis=1)\n",
    "    predicted_labels = np.argmax(ffn.predictions, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce evaluation in a new instance of model w/ same weights\n",
    "\n",
    "We load the model, with weights and data included, and evaluate and predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn2 = tm.models.EstimatorFfn()\n",
    "ffn2.load_model(fn=f'{indir}saved_model')\n",
    "print(ffn2.evaluate(test_only=True))\n",
    "# saves to ffn2.predictions\n",
    "ffn2.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Residuals\n",
    "This function outputs a MatPlotLib histogram of the residuals of the given antigen index (here `0`) over the test data. `predict()` must be called first to generate our test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.plot_residuals(antigen_idx=0, target_ids=target_ids)\n",
    "ffn.compare_preds(antigen_idx=0, target_ids=target_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jboesen tcellmatch",
   "language": "python",
   "name": "jboesen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
