{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10X ``tcellmatch`` Tutorial - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run the previous notebook 10x_dataset_preprocessing.ipynb before running this notebook. It also contains details on the dataset used in this tutorial and how it is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Third party libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np  \n",
    "import wandb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utils\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Custom/local \n",
    "import tcellmatch.api as tm\n",
    "\n",
    "# ML models\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is saved when running the previous notebook ``10x_dataset_preprocessing.ipynb.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = tm.models.EstimatorFfn()\n",
    "indir = '../tutorial_data/'\n",
    "data = np.load(f\"{indir}ffn_data_big_c.npz\")\n",
    "ffn.x_train = data[\"x_train\"]\n",
    "ffn.covariates_train = data[\"covariates_train\"]\n",
    "ffn.y_train = data[\"y_train\"]\n",
    "ffn.x_test = data[\"x_test\"]\n",
    "ffn.covariates_test = data[\"covariates_test\"]\n",
    "ffn.y_test = data[\"y_test\"]\n",
    "ffn.clone_train = data[\"clone_train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices are used to split the data into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.load_idx(f'{indir}SAVED_IDX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "Here, we can build two models: a BiLSTM-based or self-attention-based model, detailed above.\n",
    "\n",
    "Loss has the following possible parameters\n",
    "\n",
    "1. Discrete\n",
    "    1. Binary Crossentropy (param \"bce\")\n",
    "    2. Weighted Binary Crossentropy (param \"wbce\")\n",
    "    3. Categorical Crossentropy (param \"cce\")\n",
    "2. Continuous\n",
    "    1. MMD (param \"mmd\")\n",
    "    2. Mean Squared error (param \"mse\")\n",
    "    3. Poisson (param \"pois\")\n",
    "    \n",
    "Calling one of these creates the model and sets the ffn.model attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bilstm'\n",
    "USE_BIND_COUNTS = True\n",
    "LR = 0.005\n",
    "depth_final_dense = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name.lower() == 'self-attention':\n",
    "    # n_layers = 5\n",
    "    # ffn.build_self_attention(\n",
    "    #     aa_embedding_dim=0,\n",
    "    #     attention_size=[5] * n_layers, # hidden size of each attention layer\n",
    "    #     attention_heads=[4] * n_layers, # number of heads at each layer\n",
    "    #     optimizer='adam',\n",
    "    #     lr=0.001,\n",
    "    #     loss='pois' if USE_BIND_COUNTS else 'wbce',\n",
    "    #     label_smoothing=0\n",
    "    # )\n",
    "    attention_size = [128, 128]\n",
    "    attention_heads = [16, 16]\n",
    "    params = {\"attention_size\" : attention_size,\n",
    "              \"attention_heads\" : attention_heads}\n",
    "    ffn.build_self_attention(\n",
    "        residual_connection=True,\n",
    "        aa_embedding_dim=26,\n",
    "        # hidden size of each attention layer\n",
    "        attention_size=attention_size,\n",
    "        # number of heads at each layer\n",
    "        use_covariates=False,\n",
    "        attention_heads=attention_heads,\n",
    "        depth_final_dense=3,\n",
    "        optimizer='adam',\n",
    "        lr=LR,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wbce',\n",
    "        label_smoothing=0\n",
    "    )\n",
    "\n",
    "elif model_name.lower() == 'bilstm':\n",
    "    # ffn.build_bilstm(\n",
    "    #     topology = [10, 10, 10, 10], # The depth of each bilstm layer (length of feature vector)\n",
    "    #     # Dimension of the linear amino acid embedding, ie number of 1x1 convolutional filters.\n",
    "    #     # set to input dimension if aa_embedding_dim==0.\n",
    "    #     aa_embedding_dim=0,\n",
    "    #     optimizer='adam',\n",
    "    #     lr=0.001,\n",
    "    #     loss='pois' if USE_BIND_COUNTS else 'wcbe',\n",
    "    #     label_smoothing=0,\n",
    "    #     use_covariates=False,        # whether to assume covariates in model architecture\n",
    "    #     one_hot_y=not USE_BIND_COUNTS # whether we are predicting max binding categorical or bind counts\n",
    "    # )\n",
    "    topology = [32, 32, 32]#[128, 128, 128, 128]\n",
    "    params = {\"topology\" : topology}\n",
    "    ffn.build_bilstm(\n",
    "        # The depth of each bilstm layer (length of feature vector)\n",
    "        topology = topology,\n",
    "        # Dimension of the linear amino acid embedding, ie number of 1x1 convolutional filters.\n",
    "        # set to input dimension if aa_embedding_dim==0.\n",
    "        aa_embedding_dim=0,\n",
    "        # depth_final_dense=depth_final_dense,\n",
    "        optimizer='adam',\n",
    "        lr=LR,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wcbe',\n",
    "        label_smoothing=0,\n",
    "        depth_final_dense=depth_final_dense,\n",
    "        # whether to assume covariates in model architecture\n",
    "        use_covariates=False,\n",
    "        # whether we are predicting max binding categorical\n",
    "        # or binding counts\n",
    "        one_hot_y=not USE_BIND_COUNTS\n",
    "    )\n",
    "\n",
    "elif model_name.lower() == 'bigru':\n",
    "    ffn.build_bigru(\n",
    "        topology=[10, 10, 10],\n",
    "        aa_embedding_dim=0,\n",
    "        lr=0.005,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wbce',\n",
    "        optimize_for_gpu= True,\n",
    "    )\n",
    "elif model_name.lower() == 'cnn':\n",
    "    ffn.build_conv(\n",
    "        n_conv_layers = 3,\n",
    "        depth_final_dense = 3,\n",
    "        filter_widths = [3, 5, 3],  # Filter widths for the three convolutional layers\n",
    "        filters = [16, 32, 64],  # Output channels for the three convolutional layers\n",
    "        pool_sizes = [2] * 3,  # Size of the pooling window\n",
    "        pool_strides = [2] * 3,  # Stride for moving the pooling window,\n",
    "        loss='pois' if USE_BIND_COUNTS else 'wbce',\n",
    "    )\n",
    "ffn.model = ffn.model.to(device) \n",
    "\n",
    "for param in ffn.model.parameters():\n",
    "    param = param.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Train this model for 2 epochs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.idx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training...\n",
      "Number of observations in evaluation data: 7893\n",
      "Number of observations in training data: 71103\n",
      "Epoch 0, loss: 1.33\n",
      "Epoch 1, loss: 1.18\n",
      "Epoch 2, loss: 1.17\n",
      "Epoch 3, loss: 1.14\n",
      "Epoch 4, loss: 1.1\n",
      "Epoch 5, loss: 1.06\n",
      "Epoch 6, loss: 1.02\n",
      "Epoch 7, loss: 0.99\n",
      "Epoch 8, loss: 0.98\n",
      "Epoch 9, loss: 0.96\n",
      "Epoch 10, loss: 0.96\n",
      "Epoch 11, loss: 0.93\n",
      "Epoch 12, loss: 0.92\n",
      "Epoch 13, loss: 0.91\n",
      "Epoch 14, loss: 0.9\n",
      "Epoch 15, loss: 0.88\n",
      "Epoch 16, loss: 0.89\n",
      "Epoch 17, loss: 0.87\n",
      "Epoch 18, loss: 0.86\n",
      "Epoch 19, loss: 0.84\n",
      "Epoch 20, loss: 0.83\n",
      "Epoch 21, loss: 0.82\n",
      "Epoch 22, loss: 0.82\n",
      "Epoch 23, loss: 0.81\n",
      "Epoch 24, loss: 0.8\n",
      "Epoch 25, loss: 0.88\n",
      "Epoch 26, loss: 0.83\n",
      "Epoch 27, loss: 0.8\n",
      "Epoch 28, loss: 0.78\n",
      "Epoch 29, loss: 0.78\n",
      "Epoch 30, loss: 0.77\n",
      "Epoch 31, loss: 0.76\n",
      "Epoch 32, loss: 0.76\n",
      "Epoch 33, loss: 0.76\n",
      "Epoch 34, loss: 0.75\n",
      "Epoch 35, loss: 0.74\n",
      "Epoch 36, loss: 0.74\n",
      "Epoch 37, loss: 0.74\n",
      "Epoch 38, loss: 0.74\n",
      "Epoch 39, loss: 0.72\n",
      "Epoch 40, loss: 0.72\n",
      "Epoch 41, loss: 0.73\n",
      "Epoch 42, loss: 0.72\n",
      "Epoch 43, loss: 0.72\n",
      "Epoch 44, loss: 0.71\n",
      "Epoch 45, loss: 0.71\n",
      "Epoch 46, loss: 0.7\n",
      "Epoch 47, loss: 0.71\n",
      "Epoch 48, loss: 0.7\n",
      "Epoch 49, loss: 0.7\n",
      "Epoch 50, loss: 0.7\n",
      "Epoch 51, loss: 0.7\n",
      "Epoch 52, loss: 0.71\n",
      "Epoch 53, loss: 0.69\n",
      "Epoch 54, loss: 0.69\n",
      "Epoch 55, loss: 0.68\n",
      "Epoch 56, loss: 0.67\n",
      "Epoch 57, loss: 0.67\n",
      "Epoch 58, loss: 0.67\n",
      "Epoch 59, loss: 0.68\n",
      "Epoch 60, loss: 0.69\n",
      "Epoch 61, loss: 0.68\n",
      "Epoch 62, loss: 0.67\n",
      "Epoch 63, loss: 1.18\n",
      "Epoch 64, loss: 1.16\n",
      "Epoch 65, loss: 1.11\n",
      "Epoch 66, loss: 1.09\n",
      "Epoch 67, loss: 1.06\n",
      "Epoch 68, loss: 1.04\n",
      "Epoch 69, loss: 1.03\n",
      "Epoch 70, loss: 1.01\n",
      "Epoch 71, loss: 1.01\n",
      "Epoch 72, loss: 0.98\n",
      "Epoch 73, loss: 0.99\n",
      "Epoch 74, loss: 0.98\n",
      "Epoch 75, loss: 0.97\n",
      "Epoch 76, loss: 0.95\n",
      "Epoch 77, loss: 0.94\n",
      "Epoch 78, loss: 0.99\n",
      "Epoch 79, loss: 0.94\n",
      "Epoch 80, loss: 0.92\n",
      "Epoch 81, loss: 0.93\n"
     ]
    }
   ],
   "source": [
    "train_curve, val_curve, antigen_loss, antigen_loss_val = ffn.train(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=batch_size,\n",
    "    allow_early_stopping=False,\n",
    "    print_loss=True,\n",
    "    lr_schedule_factor=0.99999,\n",
    "    use_wandb=False,\n",
    "    patience=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curves\n",
    "plt.plot(train_curve, label='Train Curve')\n",
    "plt.plot(val_curve, label='Validation Curve')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Curves')\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.model.get_embeddings(torch.ones(1, 1, 40, 26, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set\n",
    "This evaluates the data and returns binary and custom (i.e., based on how the model was built above) loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.evaluate(\n",
    "    # given k, returns loss only over the kth antigen in test\n",
    "    antigen_col=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%-m-%-d--%-H:%M\")\n",
    "os.makedirs(f'{indir}saved_model-{timestamp}', exist_ok=True)\n",
    "# save_yhat means save predictions\n",
    "ffn.save_model_full(f'{indir}saved_model-{timestamp}', save_yhat=True, save_train_data=False)\n",
    "print(f'Saved model to {indir}saved_model-{timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate/Compare the Model\n",
    "We predict the labels on test data and store to ``ffn.predictions``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.predict()\n",
    "ffn.predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Confusion Matrix\n",
    "This is only applicable, of course, if you've used the one-hot encoded maximum binding y-data. If it is, you can use this to also compare with the original tcellmatch in torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_BIND_COUNTS:\n",
    "    true_labels = np.argmax(ffn.y_test, axis=1)\n",
    "    predicted_labels = np.argmax(ffn.predictions, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce evaluation in a new instance of model w/ same weights\n",
    "We load the model, with weights and data included, and evaluate and predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn2 = tm.models.EstimatorFfn()\n",
    "ffn2.load_model_full(fn=f'{indir}saved_model-{timestamp}', load_train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -> fix this\n",
    "# ffn2.model.embed_attention_layers = torch.nn.ModuleList([l.to(device=device) for l in ffn2.model.embed_attention_layers])\n",
    "# ffn2.model.linear_layers = torch.nn.ModuleList([l.to(device=device) for l in ffn2.model.linear_layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ffn2.evaluate(test_only=True))\n",
    "ffn2.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with trivial prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lmd = torch.nn.PoissonNLLLoss(full=True, reduction='none')\n",
    "\n",
    "log_p_yhat = -p_lmd(torch.Tensor(ffn.predictions), torch.Tensor(ffn.y_test))\n",
    "trivial_pred = ffn.y_test.mean(axis=0)\n",
    "trivial_pred_vec = trivial_pred.copy()\n",
    "trivial_pred = np.expand_dims(trivial_pred, axis=0)  # Add a new axis at the beginning\n",
    "trivial_pred = np.repeat(trivial_pred, log_p_yhat.shape[0], axis=0)\n",
    "print('trivial pred shape', trivial_pred.shape)\n",
    "trivial_log_p = -p_lmd(torch.Tensor(trivial_pred), torch.Tensor(ffn.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffn.x_train[ffn.idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_yhat = log_p_yhat.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_log_p = trivial_log_p.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10, 5])\n",
    "plt.title(\"testing data\")\n",
    "ax.bar(np.arange(len(log_p_yhat))-0.25, log_p_yhat, width=0.5, label='model')\n",
    "ax.bar(np.arange(len(trivial_log_p))+0.25, trivial_log_p, width=0.5, label='trivial')\n",
    "# plt.ylim(-10, 0)\n",
    "plt.ylabel(\"average log probability\")\n",
    "plt.xlabel(\"antigen\")\n",
    "ax.legend()\n",
    "\n",
    "# wandb.log({\"test antigen liks to trivial\": wandb.Image(plt)})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 5])\n",
    "plt.title(\"testing data\")\n",
    "plt.plot([-10, len(log_p_yhat)+10], [1, 1], color='grey')\n",
    "ax.bar(np.arange(len(log_p_yhat)), log_p_yhat / trivial_log_p, width=0.5)\n",
    "plt.ylabel(\"relative average log probability\")\n",
    "plt.xlabel(\"antigen\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, len(log_p_yhat)-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot actual vs. predicted counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "antigen = 14\n",
    "plt.scatter(ffn.y_test[:, antigen], np.exp(ffn.predictions[:, antigen]), label='Data Points', color='blue')\n",
    "\n",
    "# Plot the equation x = y\n",
    "line = np.linspace(1, np.max(ffn.y_test), num=1000)\n",
    "slope, intercept = np.polyfit(ffn.y_test[:, antigen],  np.exp(ffn.predictions[:, antigen]), deg=1)\n",
    "line_of_best_fit = slope * line + intercept\n",
    "plt.plot(line, label='Best possible', color='yellow')\n",
    "plt.plot(line_of_best_fit, label='predictions best fit', color='green')\n",
    "plt.axhline(y=trivial_pred_vec[antigen], color='orange', linestyle='--', label='Trivial Prediction')\n",
    "print('Line of best fit slope: ', slope)\n",
    "# Set labels and title\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('ŷ')\n",
    "plt.title('Scatter Plot of y vs ŷ')\n",
    "# plt.ylim(0, 25)  # Change the range according to your preference\n",
    "# plt.xlim(0, 25)  # Change the range according to your preference\n",
    "plt.xscale('log')    # Set x-axis to logarithmic scale\n",
    "plt.yscale('log')    # Set y-axis to logarithmic scale\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Residuals\n",
    "This function outputs a MatPlotLib histogram of the residuals of the given antigen index (here 0) over the test data. predict() must be called first to generate our test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [collapsed] Import list of target binders and non-binders\n",
    "target_ids = [\n",
    "    'A0101_VTEHDTLLY_IE-1_CMV',\n",
    "    'A0201_KTWGQYWQV_gp100_Cancer',\n",
    "    'A0201_ELAGIGILTV_MART-1_Cancer',\n",
    "    'A0201_CLLWSFQTSA_Tyrosinase_Cancer',\n",
    "    'A0201_IMDQVPFSV_gp100_Cancer',\n",
    "    'A0201_SLLMWITQV_NY-ESO-1_Cancer',\n",
    "    'A0201_KVAELVHFL_MAGE-A3_Cancer',\n",
    "    'A0201_KVLEYVIKV_MAGE-A1_Cancer',\n",
    "    'A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase',\n",
    "    'A0201_LLDFVRFMGV_EBNA-3B_EBV',\n",
    "    'A0201_LLMGTLGIVC_HPV-16E7_82-91',\n",
    "    'A0201_CLGGLLTMV_LMP-2A_EBV',\n",
    "    'A0201_YLLEMLWRL_LMP1_EBV',\n",
    "    'A0201_FLYALALLL_LMP2A_EBV',\n",
    "    'A0201_GILGFVFTL_Flu-MP_Influenza',\n",
    "    'A0201_GLCTLVAML_BMLF1_EBV',\n",
    "    'A0201_NLVPMVATV_pp65_CMV',\n",
    "    'A0201_ILKEPVHGV_RT_HIV',\n",
    "    'A0201_FLASKIGRLV_Ca2-indepen-Plip-A2',\n",
    "    'A2402_CYTWNQMNL_WT1-(235-243)236M_Y',\n",
    "    'A0201_RTLNAWVKV_Gag-protein_HIV',\n",
    "    'A0201_KLQCVDLHV_PSA146-154',\n",
    "    'A0201_LLFGYPVYV_HTLV-1',\n",
    "    'A0201_SLFNTVATL_Gag-protein_HIV',\n",
    "    'A0201_SLYNTVATLY_Gag-protein_HIV',\n",
    "    'A0201_SLFNTVATLY_Gag-protein_HIV',\n",
    "    'A0201_RMFPNAPYL_WT-1',\n",
    "    'A0201_YLNDHLEPWI_BCL-X_Cancer',\n",
    "    'A0201_MLDLQPETT_16E7_HPV',\n",
    "    'A0301_KLGGALQAK_IE-1_CMV',\n",
    "    'A0301_RLRAEAQVK_EMNA-3A_EBV',\n",
    "    'A0301_RIAAWMATY_BCL-2L1_Cancer',\n",
    "    'A1101_IVTDFSVIK_EBNA-3B_EBV',\n",
    "    'A1101_AVFDRKSDAK_EBNA-3B_EBV',\n",
    "    'B3501_IPSINVHHY_pp65_CMV',\n",
    "    'A2402_AYAQKIFKI_IE-1_CMV',\n",
    "    'A2402_QYDPVAALF_pp65_CMV',\n",
    "    'B0702_QPRAPIRPI_EBNA-6_EBV',\n",
    "    'B0702_TPRVTGGGAM_pp65_CMV',\n",
    "    'B0702_RPPIFIRRL_EBNA-3A_EBV',\n",
    "    'B0702_RPHERNGFTVL_pp65_CMV',\n",
    "    'B0801_RAKFKQLL_BZLF1_EBV',\n",
    "    'B0801_ELRRKMMYM_IE-1_CMV',\n",
    "    'B0801_FLRGRAYGL_EBNA-3A_EBV',\n",
    "    'A0101_SLEGGGLGY_NC',\n",
    "    'A0101_STEGGGLAY_NC',\n",
    "    'A0201_ALIAPVHAV_NC',\n",
    "    'A2402_AYSSAGASI_NC',\n",
    "    'B0702_GPAESAAGL_NC',\n",
    "    'NR(B0801)_AAKGRGAAL_NC',\n",
    "]\n",
    "nc_cols = [\n",
    "    'A0101_SLEGGGLGY_NC',\n",
    "    'A0101_STEGGGLAY_NC',\n",
    "    'A0201_ALIAPVHAV_NC',\n",
    "    'A2402_AYSSAGASI_NC',\n",
    "    'B0702_GPAESAAGL_NC',\n",
    "    'NR(B0801)_AAKGRGAAL_NC'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn.plot_residuals(antigen_idx=41, target_ids=target_ids)\n",
    "ffn.compare_preds(antigen_idx=41, target_ids=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jboesen tcellmatch",
   "language": "python",
   "name": "jboesen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
